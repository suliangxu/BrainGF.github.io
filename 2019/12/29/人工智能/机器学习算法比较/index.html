<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ML,">










<meta name="description" content="本文主要回顾下几个常用算法的适应场景及其优缺点！（提示：部分内容摘自网络）。 主要来自博客：http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/ 机器学习算法太多了，分类、回归、聚类、推荐、图像识别领域等等，要想找到一个合适算法真的不容易，所以在实际应用中，我们一般都是采用启发式学习方">
<meta name="keywords" content="ML">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法比较">
<meta property="og:url" content="http://yoursite.com/2019/12/29/人工智能/机器学习算法比较/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="本文主要回顾下几个常用算法的适应场景及其优缺点！（提示：部分内容摘自网络）。 主要来自博客：http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/ 机器学习算法太多了，分类、回归、聚类、推荐、图像识别领域等等，要想找到一个合适算法真的不容易，所以在实际应用中，我们一般都是采用启发式学习方">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://www.csuldw.com/assets/articleImg/bias_variance.png">
<meta property="og:updated_time" content="2020-02-13T08:18:45.317Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法比较">
<meta name="twitter:description" content="本文主要回顾下几个常用算法的适应场景及其优缺点！（提示：部分内容摘自网络）。 主要来自博客：http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/ 机器学习算法太多了，分类、回归、聚类、推荐、图像识别领域等等，要想找到一个合适算法真的不容易，所以在实际应用中，我们一般都是采用启发式学习方">
<meta name="twitter:image" content="http://www.csuldw.com/assets/articleImg/bias_variance.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/12/29/人工智能/机器学习算法比较/">





  <title>机器学习算法比较 | Hexo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/29/人工智能/机器学习算法比较/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习算法比较</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-29T17:08:43+08:00">
                2019-12-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/人工智能/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要回顾下几个常用算法的适应场景及其优缺点！（提示：部分内容摘自网络）。</p>
<p>主要来自博客：<a href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/" target="_blank" rel="noopener">http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/</a></p>
<p>机器学习算法太多了，分类、回归、聚类、推荐、图像识别领域等等，要想找到一个合适算法真的不容易，所以在实际应用中，我们一般都是采用启发式学习方式来实验。通常最开始我们都会选择大家普遍认同的算法，诸如SVM，GBDT，Adaboost，现在深度学习很火热，神经网络也是一个不错的选择。假如你在乎精度（accuracy）的话，最好的方法就是通过<strong><em>交叉验证（cross-validation）</em></strong>对各个算法一个个地进行测试，进行比较，然后调整参数确保每个算法达到最优解，最后选择最好的一个。</p>
<p>但是如果你只是在寻找一个“足够好”的算法来解决你的问题，或者这里有些技巧可以参考，下面来分析下各个算法的优缺点，基于算法的优缺点，更易于我们去选择它。</p>
<h3 id="一、偏差与方差"><a href="#一、偏差与方差" class="headerlink" title="一、偏差与方差"></a>一、偏差与方差</h3><p>在统计学中，一个模型好坏，是根据偏差和方差来衡量的，所以我们先来普及一下偏差(bias)和方差(variance)：</p>
<ul>
<li>偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。</li>
</ul>
<p>​        $Bias[\hat{f(x)}]=E[\hat{f(x)}]−f(x)$</p>
<ul>
<li>方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。</li>
</ul>
<p>$Var[\hat{f(x)}] = E[(\hat{f(x)} - E[{\hat{f(x)])}}^2]$</p>
<p>模型的真实误差是两者之和，如公式<a href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/#mjx-eqn-3" target="_blank" rel="noopener">(3)</a>：</p>
<p>$E[(y−\hat{f(x)})^2]=Bias[\hat{f(x)}]^2+Var[\hat{f(x)}]+σ^2$</p>
<p>通常情况下，如果是小训练集，高偏差/低方差的分类器（例如，朴素贝叶斯NB）要比低偏差/高方差大分类的优势大（例如，KNN），因为后者会发生过拟合（overfiting）。然而，随着你训练集的增长，模型对于原数据的预测能力就越好，偏差就会降低，此时低偏差/高方差的分类器就会渐渐的表现其优势（因为它们有较低的渐近误差），而高偏差分类器这时已经不足以提供准确的模型了。</p>
<p>当然，你也可以认为这是生成模型（如NB）与判别模型（如KNN）的一个区别。</p>
<p><strong><em>为什么说朴素贝叶斯是高偏差低方差?</em></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">以下内容引自知乎：</span><br><span class="line"></span><br><span class="line">首先，假设你知道训练集和测试集的关系。简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？</span><br><span class="line"></span><br><span class="line">由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。</span><br><span class="line"></span><br><span class="line">在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为Error = Bias + Variance。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。</span><br><span class="line"></span><br><span class="line">所以，这样就容易分析朴素贝叶斯了。它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会Bias部分大于Variance部分，也就是说高偏差而低方差。</span><br><span class="line"></span><br><span class="line">在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。</span><br></pre></td></tr></table></figure>

<p>偏差、方差、模型复杂度三者之间的关系使用下图表示会更容易理解：</p>
<p><img src="http://www.csuldw.com/assets/articleImg/bias_variance.png" alt="img"></p>
<p>当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。</p>
<h3 id="二、常见算法的优缺点"><a href="#二、常见算法的优缺点" class="headerlink" title="二、常见算法的优缺点"></a>二、常见算法的优缺点</h3><h4 id="1、朴素贝叶斯"><a href="#1、朴素贝叶斯" class="headerlink" title="1、朴素贝叶斯"></a>1、朴素贝叶斯</h4><p>朴素贝叶斯属于<strong>生成式模型</strong>（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。</p>
<p>即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。</p>
<p><strong>优点</strong>：</p>
<ul>
<li>朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。</li>
<li>对小规模的数据表现很好，能个处理多分类任务，适合增量式训练；</li>
<li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>需要计算先验概率；</li>
<li>分类决策存在错误率；</li>
<li>对输入数据的表达形式很敏感。</li>
</ul>
<hr>
<h4 id="2-Logistic-Regression"><a href="#2-Logistic-Regression" class="headerlink" title="2.Logistic Regression"></a>2.Logistic Regression</h4><p>逻辑回归属于<strong><em>判别式模型</em></strong>，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。</p>
<p>如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。</p>
<p><strong>Sigmoid函数</strong>：表达式为公式</p>
<p>​            $f(x) = \frac{1}{1+e^{-x}}$</p>
<p><strong>优点：</strong></p>
<ul>
<li>实现简单，广泛的应用于工业问题上；</li>
<li>分类时计算量非常小，速度很快，存储资源低；</li>
<li>便利的观测样本概率分数；</li>
<li>对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>当特征空间很大时，逻辑回归的性能不是很好；</li>
<li>容易<strong>欠拟合</strong>，一般准确度不太高</li>
<li>不能很好地处理大量多类特征或变量；</li>
<li>只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须<strong>线性可分</strong>；</li>
<li>对于非线性特征，需要进行转换；</li>
</ul>
<hr>
<h4 id="3、线性回归"><a href="#3、线性回归" class="headerlink" title="3、线性回归"></a>3、线性回归</h4><p>线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用<strong>梯度下降法</strong>对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为：</p>
<p>$\hat{w} = (X^TX)^{-1}X^Ty$</p>
<p>而在LWLR（局部加权线性回归）中，参数的计算表达式为：</p>
<p>$\hat{w} = (X^TWX)^{-1}X^TWy$</p>
<p>由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。</p>
<p><strong>优点</strong>： 实现简单，计算简单；</p>
<p><strong>缺点</strong>： 不能拟合非线性数据.</p>
<hr>
<h4 id="4、KNN"><a href="#4、KNN" class="headerlink" title="4、KNN"></a>4、KNN</h4><p>KNN即最近邻算法，其主要过程为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</span><br><span class="line">2. 对上面所有的距离值进行排序(升序)；</span><br><span class="line">3. 选前k个最小距离的样本；</span><br><span class="line">4. 根据这k个样本的标签进行投票，得到最后的分类类别；</span><br></pre></td></tr></table></figure>

<p>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。</p>
<p><strong>优点</strong></p>
<ul>
<li>理论成熟，思想简单，既可以用来做分类也可以用来做回归；</li>
<li>可用于非线性分类；</li>
<li>训练时间复杂度为O(n)；</li>
<li>对数据没有假设，准确度高，对outlier不敏感；</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>计算量大（体现在距离计算上）；</li>
<li>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差；</li>
<li>需要大量内存；</li>
</ul>
<hr>
<h4 id="5、决策树"><a href="#5、决策树" class="headerlink" title="5、决策树"></a>5、决策树</h4><p>决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。</p>
<p>另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。</p>
<p>决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。</p>
<p>信息熵的计算公式如下:</p>
<p>$H=-\sum_{i=1}^n p(xi)log_{2}{p(xi)}$</p>
<p>其中的n代表有n个分类类别（比如假设是二类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。</p>
<p>现在选中一个属性 $xi$ 用来进行分枝，此时分枝规则是：如果       $xi=v$  的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵 $H’ =p1 * H1+p2 * H2$ ,则此时的信息增益 $ΔH = H - H’$ 。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。</p>
<p><strong>决策树自身的优点</strong></p>
<ul>
<li>计算简单，易于理解，可解释性强；</li>
<li>比较适合处理有缺失属性的样本；</li>
<li>能够处理不相关的特征；</li>
<li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>容易发生过拟合（随机森林可以很大程度上减少过拟合）；</li>
<li>忽略了数据之间的相关性；</li>
<li>对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）。</li>
</ul>
<h5 id="1-Adaboosting"><a href="#1-Adaboosting" class="headerlink" title="(1) Adaboosting"></a>(1) Adaboosting</h5><p>Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。有兴趣的同学可以阅读下笔者后面写的这篇文章<a href="http://www.csuldw.com/2016/08/28/2016-08-28-adaboost-algorithm-theory/" target="_blank" rel="noopener">Adaboost - 新的角度理解权值更新策略</a>.下面总结下它的优缺点。</p>
<p><strong>优点</strong></p>
<ul>
<li>Adaboost是一种有很高精度的分类器。</li>
<li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li>
<li>当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。</li>
<li>简单，不用做特征筛选。</li>
<li>不易发生overfitting。</li>
</ul>
<p>关于随机森林和GBDT等组合算法，参考这篇文章：[机器学习-组合算法总结](<a href="http://www.csuldw.com/2015/07/22/2015-07-22" target="_blank" rel="noopener">http://www.csuldw.com/2015/07/22/2015-07-22</a>  ensemble/)</p>
<p><strong>缺点：</strong></p>
<p>​    对outlier比较敏感</p>
<hr>
<h4 id="6、SVM"><a href="#6、SVM" class="headerlink" title="6、SVM"></a>6、SVM</h4><p>支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。</p>
<p><strong>优点</strong></p>
<ul>
<li>可以解决高维问题，即大型特征空间；</li>
<li>能够处理非线性特征的相互作用；</li>
<li>无需依赖整个数据；</li>
<li>可以提高泛化能力；</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>当观测样本很多时，效率并不是很高；</li>
<li>对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；</li>
<li>对缺失数据敏感；</li>
</ul>
<p>对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：</p>
<ul>
<li>第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；</li>
<li>第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；</li>
<li>第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。</li>
</ul>
<p>对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。</p>
<hr>
<h4 id="7、人工神经网络"><a href="#7、人工神经网络" class="headerlink" title="7、人工神经网络"></a>7、人工神经网络</h4><p><strong>优点：</strong></p>
<ul>
<li>分类的准确度高；</li>
<li>并行分布处理能力强,分布存储及学习能力强，</li>
<li>对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系；</li>
<li>具备联想记忆的功能。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；</li>
<li>不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；</li>
<li>学习时间过长,甚至可能达不到学习的目的。</li>
</ul>
<hr>
<h4 id="8、K-Means-聚类"><a href="#8、K-Means-聚类" class="headerlink" title="8、K - Means 聚类"></a>8、K - Means 聚类</h4><p>之前笔者写过一篇关于K-Means聚类的文章，参见<a href="http://www.csuldw.com/2015/06/03/2015-06-03-ml-algorithm-K-means/" target="_blank" rel="noopener">机器学习算法-K-means聚类</a>。关于K-Means的推导，里面可是有大学问的，蕴含着强大的EM思想。</p>
<p><strong>优点</strong></p>
<ul>
<li>算法简单，容易实现 ；</li>
<li>对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;nk&lt;&lt;n。这个算法通常局部收敛。</li>
<li>算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>对数据类型要求较高，适合数值型数据；</li>
<li>可能收敛到局部最小值，在大规模数据上收敛较慢 </li>
<li>K值比较难以选取；</li>
<li>对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；</li>
<li>不适合于发现非凸面形状的簇，或者大小差别很大的簇。</li>
<li>对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。</li>
</ul>
<h3 id="三、算法选择参考"><a href="#三、算法选择参考" class="headerlink" title="三、算法选择参考"></a>三、算法选择参考</h3><p>之前笔者翻译过一些国外的文章，其中有一篇文章中给出了一个简单的算法选择技巧：</p>
<ol>
<li>首次应该选择的就是逻辑回归，如果它的效果不怎么样，那么可以将它的结果作为基准来参考，在基础上与其他算法进行比较；</li>
<li>然后试试决策树（随机森林）看看是否可以大幅度提升你的模型性能。即便最后你并没有把它当做为最终模型，你也可以使用随机森林来移除噪声变量，做特征选择；</li>
<li>如果特征的数量和观测样本特别多，那么当资源和时间充足时（这个前提很重要），使用SVM不失为一种选择。</li>
</ol>
<p>通常情况下：【GBDT&gt;=SVM&gt;=RF&gt;=Adaboost&gt;=Other…】，现在深度学习很热门，很多领域都用到，它是以神经网络为基础的，目前笔者自己也在学习，只是理论知识不扎实，理解的不够深入，这里就不做介绍了，希望以后可以写一片抛砖引玉的文章。</p>
<p>算法固然重要，<strong>但好的数据却要优于好的算法</strong>，设计优良特征是大有裨益的。假如你有一个超大数据集，那么无论你使用哪种算法可能对分类性能都没太大影响（此时就可以根据速度和易用性来进行抉择）。</p>
<h3 id="四、参考文献"><a href="#四、参考文献" class="headerlink" title="四、参考文献"></a>四、参考文献</h3><p>[1] <a href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff</a></p>
<p>[2] <a href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/" target="_blank" rel="noopener">http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/</a></p>
<p>[3]<a href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/" target="_blank" rel="noopener">http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ML/" rel="tag"># ML</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/15/随笔/最大间距(LeetCode164)/" rel="next" title="最大间距(LeeCode164)">
                <i class="fa fa-chevron-left"></i> 最大间距(LeeCode164)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/01/17/随笔/pip命令的使用/" rel="prev" title="pip命令的使用">
                pip命令的使用 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="John Doe">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">92</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://suliangxu.github.io" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1193135584@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、偏差与方差"><span class="nav-text">一、偏差与方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、常见算法的优缺点"><span class="nav-text">二、常见算法的优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、朴素贝叶斯"><span class="nav-text">1、朴素贝叶斯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Logistic-Regression"><span class="nav-text">2.Logistic Regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、线性回归"><span class="nav-text">3、线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4、KNN"><span class="nav-text">4、KNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5、决策树"><span class="nav-text">5、决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Adaboosting"><span class="nav-text">(1) Adaboosting</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6、SVM"><span class="nav-text">6、SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7、人工神经网络"><span class="nav-text">7、人工神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8、K-Means-聚类"><span class="nav-text">8、K - Means 聚类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、算法选择参考"><span class="nav-text">三、算法选择参考</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、参考文献"><span class="nav-text">四、参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  



  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
